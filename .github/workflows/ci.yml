name: Continuous Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.9'

jobs:
  # Code Quality and Linting
  code-quality:
    runs-on: ubuntu-latest
    name: Code Quality Checks
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy bandit safety
        pip install -r requirements.txt
        
    - name: Run Black (Code Formatting)
      run: |
        black --check --diff src/ tests/
        
    - name: Run isort (Import Sorting)
      run: |
        isort --check-only --diff src/ tests/
        
    - name: Run Flake8 (Linting)
      run: |
        flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503
        
    - name: Run MyPy (Type Checking)
      run: |
        mypy src/ --ignore-missing-imports --no-strict-optional
        
    - name: Run Bandit (Security Scanning)
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ --severity-level medium
        
    - name: Run Safety (Dependency Vulnerability Check)
      run: |
        safety check --json --output safety-report.json || true
        safety check
        
    - name: Upload Security Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Unit Testing
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10']
        
    name: Tests (Python ${{ matrix.python-version }})
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-mock httpx
        pip install -r requirements.txt
        
    - name: Create necessary directories
      run: |
        mkdir -p data/raw data/processed data/external data/interim
        mkdir -p logs mlruns
        
    - name: Run tests with coverage
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --cov-report=term
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml

  # Data and Model Validation
  data-model-validation:
    runs-on: ubuntu-latest
    name: Data and Model Validation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create necessary directories
      run: |
        mkdir -p data/raw data/processed data/external data/interim
        mkdir -p logs mlruns
        
    - name: Validate data loading
      run: |
        echo "üìä Validating data loading..."
        python -c "
        from sklearn.datasets import fetch_california_housing
        import pandas as pd
        import os
        
        # Create data directories
        os.makedirs('data/raw', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
        
        # Load and save sample data
        housing = fetch_california_housing()
        df = pd.DataFrame(housing.data, columns=housing.feature_names)
        df['target'] = housing.target
        
        # Save to CSV
        df.to_csv('data/processed/housing_data.csv', index=False)
        print(f'‚úÖ Data saved: {df.shape}')
        "
        
    - name: Validate model training
      run: |
        echo "üèãÔ∏è Validating model training..."
        python -c "
        from sklearn.datasets import fetch_california_housing
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, r2_score
        import pandas as pd
        import numpy as np
        import os
        
        # Create directories
        os.makedirs('models', exist_ok=True)
        os.makedirs('mlruns', exist_ok=True)
        
        # Load data
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        model = RandomForestRegressor(n_estimators=10, random_state=42)  # Small for CI
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        print(f'‚úÖ Model trained successfully')
        print(f'‚úÖ RMSE: {rmse:.3f}')
        print(f'‚úÖ R2 Score: {r2:.3f}')
        
        # Save model info
        with open('models/model_info.txt', 'w') as f:
            f.write(f'RMSE: {rmse:.3f}\nR2: {r2:.3f}\n')
        "
        echo "‚úÖ Model training validation completed"
        
    - name: Check MLflow artifacts
      run: |
        ls -la mlruns/ || echo "MLflow directory not found"
        
    - name: Upload MLflow artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: mlflow-artifacts
        path: mlruns/

  # Docker Build and Test
  docker:
    runs-on: ubuntu-latest
    name: Docker Build and Test
    needs: [code-quality, test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: housing-price-api:test
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Test Docker image
      run: |
        # Create necessary directories for volume mounts
        mkdir -p data/raw data/processed logs mlruns
        
        # Start the container in background
        docker run -d --name test-api \
          -p 8000:8000 \
          -v $(pwd)/data:/app/data \
          -v $(pwd)/logs:/app/logs \
          -v $(pwd)/mlruns:/app/mlruns \
          housing-price-api:test
          
        # Wait for container to start
        sleep 30
        
        # Test health endpoint
        curl -f http://localhost:8000/health || exit 1
        
        # Test root endpoint
        curl -f http://localhost:8000/ || exit 1
        
        # Stop container
        docker stop test-api
        docker rm test-api
        
    - name: Scan Docker image for vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: housing-price-api:test
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # Integration Tests
  integration:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: [docker]
    
    services:
      mlflow:
        image: python:3.9-slim
        ports:
          - 5000:5000
        options: >-
          --health-cmd "curl -f http://localhost:5000/health || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 3
        env:
          PYTHONUNBUFFERED: 1
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests pytest
        pip install -r requirements.txt
        
    - name: Start MLflow server
      run: |
        pip install mlflow
        mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db &
        sleep 10
        
    - name: Run integration tests
      run: |
        echo "üß™ Running integration tests..."
        
        # For now, we'll run basic integration tests
        # In a real scenario, this would test the full API
        
        # Test that we can import our modules
        python -c "
        try:
            import src.data.load_data
            import src.models.train_model
            print('‚úÖ Module imports successful')
        except ImportError as e:
            print(f'‚ùå Import error: {e}')
            exit(1)
        "
        
        # Test basic data loading
        python -c "
        import pandas as pd
        from sklearn.datasets import fetch_california_housing
        
        # Load sample data
        housing = fetch_california_housing()
        df = pd.DataFrame(housing.data, columns=housing.feature_names)
        df['target'] = housing.target
        
        print(f'‚úÖ Data loaded successfully: {df.shape}')
        print(f'‚úÖ Features: {list(df.columns)}')
        "
        
        echo "‚úÖ Integration tests completed"

  # Performance Tests
  performance:
    runs-on: ubuntu-latest
    name: Performance Tests
    needs: [integration]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install locust
        pip install -r requirements.txt
        
    - name: Run performance tests
      run: |
        # This would run load tests with Locust
        echo "Performance tests would run here"
        # locust -f tests/performance/locustfile.py --headless -u 10 -r 2 -t 30s --host http://localhost:8000

  # Notify on completion
  notify:
    runs-on: ubuntu-latest
    name: Notify Results
    needs: [code-quality, test, data-model-validation, docker, integration]
    if: always()
    
    steps:
    - name: Notify Success
      if: ${{ needs.code-quality.result == 'success' && needs.test.result == 'success' && needs.docker.result == 'success' }}
      run: |
        echo "‚úÖ All CI checks passed successfully!"
        
    - name: Notify Failure
      if: ${{ needs.code-quality.result == 'failure' || needs.test.result == 'failure' || needs.docker.result == 'failure' }}
      run: |
        echo "‚ùå Some CI checks failed. Please review the logs."
        exit 1